# Perseus Data Migration Scripts - DEV Environment

**Purpose:** Extract 15% sample data from SQL Server PRODUCTION and load into PostgreSQL DEV environment with full referential integrity.

**Status:** ‚úÖ Production-Ready with Automated Workflow (Version 3.0)

---

## üìã Overview

This directory contains a **fully automated data migration workflow** designed to:

1. **Extract** 15% sample from SQL Server Production (~76 tables)
2. **Maintain referential integrity** via cascading FK-aware sampling
3. **Export** to CSV files with validation
4. **Load** data into PostgreSQL in dependency order (5 tiers)
5. **Validate** referential integrity, row counts, and data checksums

**Critical Success Factors:**
- ‚úÖ **Production-safe** execution (NOLOCK hints, tempdb checks, session monitoring)
- ‚úÖ Zero orphaned FK relationships
- ‚úÖ 15% ¬±2% variance acceptable (13-17%)
- ‚úÖ P0 critical tables validated: `goo`, `fatsmurf`, `material_transition`, `transition_material`
- ‚úÖ UID-based FKs handled correctly (not integer PK/FK)

---

## üÜï Version 3.0 - Production Safety Features

### What's New

**Production-Safe SQL Scripts:**
- **Session ID Logging**: Track `@@SPID` for manual intervention if needed
- **Tempdb Space Checks**: Require 2GB+ free space before execution (prevents crashes)
- **NOLOCK Hints**: All queries use `WITH (NOLOCK)` to avoid blocking production transactions
- **Deterministic Sampling**: Replaced `ORDER BY NEWID()` with modulo-based filtering (reproducible results)
- **Enhanced Error Handling**: Comprehensive TRY/CATCH with actionable error messages

**Automated Orchestration Script:**
- **extract-data.sh**: Single command executes entire extraction + export workflow
- **Connection Management**: Credentials stored in `.env` file (not hardcoded)
- **Prerequisites Validation**: Auto-checks disk space, connectivity, tempdb before starting
- **Progress Tracking**: Real-time status updates with color-coded output
- **Cleanup Automation**: Temp tables automatically cleaned up on exit/error
- **Summary Reporting**: Execution metrics (tables, rows, duration, CSV size)

---

## üóÇÔ∏è Files in This Directory

### Orchestration Scripts

| File | Purpose | Usage |
|------|---------|-------|
| **extract-data.sh** | **Automated extraction + export** | `./extract-data.sh` |
| `.env` | SQL Server credentials (SENSITIVE) | Auto-loaded by extract-data.sh |
| `.env.example` | Template for .env setup | Copy to .env and customize |

### Extraction Scripts (SQL Server) - Version 3.0 Production-Safe

| File | Purpose | Tables | Safety Features |
|------|---------|--------|-----------------|
| `extract-tier0.sql` | Extract 15% from base tables | 32 | Session ID, tempdb check, NOLOCK, deterministic |
| `extract-tier1.sql` | Extract 15% with FK filtering | 10 | Same as tier0 + FK-aware sampling |
| `extract-tier2.sql` | Extract 15% with FK filtering | 11 | Same + workflow_step logic fix |
| `extract-tier3.sql` | Extract P0 critical + 12 tables | 12 | Same + P0 validation (goo, fatsmurf) |
| `extract-tier4.sql` | Extract P0 lineage + 11 tables | 11 | Same + UID-based FK handling |

**Total:** 76 tables extracted with production-safe cascading FK-aware sampling

### Loading Scripts (PostgreSQL)

| File | Purpose | Usage |
|------|---------|-------|
| `load-data.sh` | Orchestrate CSV loading in dependency order | `./load-data.sh` |

### Validation Scripts (PostgreSQL)

| File | Purpose | Usage |
|------|---------|-------|
| `validate-referential-integrity.sql` | Check all 121 FK constraints | `psql -f validate-referential-integrity.sql` |
| `validate-row-counts.sql` | Verify 15% sampling rate | `psql -f validate-row-counts.sql` |
| `validate-checksums.sql` | Sample-based data integrity | `psql -f validate-checksums.sql` |

---

## üöÄ Quick Start (Automated Workflow)

### Step 1: Initial Setup (One-Time)

```bash
# Navigate to data migration directory
cd scripts/data-migration

# Create .env file from template
cp .env.example .env

# Edit .env with your SQL Server credentials
vim .env  # or nano, code, etc.

# Secure the credentials file
chmod 600 .env

# Verify connectivity
./extract-data.sh --dry-run
```

**Example .env contents:**
```bash
SQL_SERVER=sqlapps
SQL_USER=sqlapps-repl
SQL_PASSWORD=your_password_here
SQL_DATABASE=perseus
SQL_TIMEOUT=1800
DATA_DIR=/tmp/perseus-data-export
```

### Step 2: Execute Extraction + Export

```bash
# Full extraction (all 5 tiers) + CSV export
./extract-data.sh

# Specific tier only
./extract-data.sh --tier 3

# Tier range
./extract-data.sh --tier 0-2

# Custom timeout (30 minutes)
./extract-data.sh --timeout 1800

# Dry-run validation (no actual execution)
./extract-data.sh --dry-run
```

**Expected Output:**
```
========================================
Perseus Data Extraction - Production Safe Mode
========================================
[INFO] Loading configuration from .env
[INFO] Verifying prerequisites...
[SUCCESS] SQL Server connectivity: OK
[SUCCESS] Tempdb free space: 5120 MB (>2000 MB required)
[SUCCESS] Local disk space: 8192 MB (>3000 MB required)
[INFO] Backing up existing CSVs to backup-20260129-161500/
[INFO] Starting tier 0 extraction...
[INFO] Session ID: 1234 (use: KILL 1234 if needed)
[SUCCESS] Tier 0 complete: 32 tables, 12,450 rows
[INFO] Starting tier 1 extraction...
...
[SUCCESS] All tiers complete!
[INFO] Exporting temp tables to CSV...
[SUCCESS] 76 CSV files exported (total: 1,250 MB)
========================================
Extraction Summary
========================================
Tables Processed: 76
Total Rows: 125,340
Total CSV Size: 1.22 GB
Execution Time: 287s (00:04:47)
Log File: logs/extract-data-20260129-161500.log
CSV Directory: /tmp/perseus-data-export/
========================================
```

### Step 3: Load Data into PostgreSQL

```bash
# Ensure PostgreSQL DEV container is running
docker ps | grep perseus-postgres-dev

# Load all CSV files in dependency order
./load-data.sh

# Load specific tier
./load-data.sh --tier 3
```

### Step 4: Validate Data Integrity

```bash
# 1. Referential Integrity (CRITICAL - must pass 100%)
psql -U perseus_admin -d perseus_dev -f validate-referential-integrity.sql

# Expected: All 121 FK constraints PASS (0 orphaned rows)

# 2. Row Count Validation (15% ¬±2%)
psql -U perseus_admin -d perseus_dev -f validate-row-counts.sql

# Expected: 13-17% variance for all tables

# 3. Checksum Validation (sample-based)
psql -U perseus_admin -d perseus_dev -f validate-checksums.sql

# Expected: Checksums match SQL Server (manual comparison)
```

---

## üìñ Detailed Documentation

### Production Safety Guarantees

**1. No Production Impact**
- All queries use `WITH (NOLOCK)` hint (no read locks)
- Deterministic sampling (no expensive `NEWID()` sorting)
- Tempdb space validated before execution (prevents out-of-space errors)
- Session ID logged for emergency kill operations

**2. Reproducible Results**
- Deterministic modulo-based sampling (same results on re-run)
- Logged execution for audit trail
- Version-controlled extraction scripts

**3. Automatic Cleanup**
- Temp tables removed on script exit/error
- Trap handlers for INT/TERM signals (Ctrl+C safe)
- CSV backups prevent data loss

### Prerequisites Checklist

Before running extract-data.sh, ensure:

- [ ] **SQL Server Access**
  - Read permissions on `perseus` database
  - `sqlcmd` and `bcp` installed on macOS
  - Network connectivity to SQL Server

- [ ] **System Resources**
  - Local disk: >3GB free space (for CSVs)
  - SQL Server tempdb: >2GB free space

- [ ] **Configuration**
  - `.env` file created with valid credentials
  - `.env` file secured (`chmod 600`)

### extract-data.sh Command Reference

```bash
# Usage
./extract-data.sh [OPTIONS]

# Options
--dry-run              Validate setup without executing
--tier N               Execute specific tier (0-4)
--tier START-END       Execute tier range (e.g., 0-2)
--timeout SECONDS      Query timeout (default: 1800)
--no-cleanup           Skip temp table cleanup (for debugging)
--help                 Show detailed usage information

# Examples
./extract-data.sh                        # Full extraction
./extract-data.sh --dry-run              # Test configuration
./extract-data.sh --tier 2               # Tier 2 only
./extract-data.sh --tier 0-2             # Tiers 0, 1, 2
./extract-data.sh --timeout 3600         # 1 hour timeout
./extract-data.sh --no-cleanup --tier 3  # Debug tier 3

# Environment Variables (override .env)
SQL_SERVER=server ./extract-data.sh      # Custom server
DATA_DIR=/data ./extract-data.sh         # Custom output dir
```

### Sampling Strategy: Production-Safe Cascading FK-Aware

**Problem:** Random 15% sampling (`ORDER BY NEWID()`) is expensive and breaks FK relationships.

**Solution:** Deterministic modulo-based sampling with FK filtering:

```sql
-- TIER 0: Deterministic 15% (no FK dependencies)
SELECT g.*
FROM dbo.goo_type g WITH (NOLOCK)
WHERE (CAST(g.goo_type_id AS BIGINT) % 7 = 0
       OR CAST(g.goo_type_id AS BIGINT) % 7 = 1);  -- ~28.6%, limited by TOP 15 PERCENT

-- TIER 1+: 15% of rows WITH valid FK values
WITH valid_goo_types AS (
    SELECT goo_type_id FROM ##perseus_tier_0_goo_type
)
SELECT g.*
FROM dbo.goo g WITH (NOLOCK)
WHERE g.goo_type_id IN (SELECT goo_type_id FROM valid_goo_types)
  AND (CAST(g.goo_id AS BIGINT) % 7 = 0
       OR CAST(g.goo_id AS BIGINT) % 7 = 1);
```

**Benefits:**
- **NOLOCK**: No read locks on production tables
- **Deterministic**: Same IDs selected on re-run (reproducible)
- **Fast**: No sorting, just modulo arithmetic (~10√ó faster than NEWID)
- **FK-Safe**: Zero orphaned relationships

### UID-Based Foreign Keys (CRITICAL)

**Tables:** `material_transition`, `transition_material`

**Challenge:** These tables use VARCHAR `uid` columns (not integer `id`) for FK references.

**Solution:**
1. Created UNIQUE indexes on `goo.uid` and `fatsmurf.uid`
2. Modified extraction scripts to filter by `uid` values:

```sql
-- Tier 4 extraction (UID-based FK filtering)
WITH valid_goo_uids AS (
    SELECT uid FROM ##perseus_tier_3_goo WHERE uid IS NOT NULL
),
valid_fatsmurf_uids AS (
    SELECT uid FROM ##perseus_tier_3_fatsmurf WHERE uid IS NOT NULL
)
SELECT mt.*
FROM dbo.material_transition mt WITH (NOLOCK)
WHERE mt.material_id IN (SELECT uid FROM valid_goo_uids)
  AND mt.transition_id IN (SELECT uid FROM valid_fatsmurf_uids)
  AND (CAST(mt.id AS BIGINT) % 7 = 0
       OR CAST(mt.id AS BIGINT) % 7 = 1);
```

**Validation:** Check `validate-referential-integrity.sql` for UID-based FK checks.

### Dependency Tiers

| Tier | Description | Tables | Example Tables |
|------|-------------|--------|----------------|
| 0 | Base tables (no FK dependencies) | 32 | `goo_type`, `unit`, `manufacturer`, `container` |
| 1 | Depend only on Tier 0 | 10 | `property`, `perseus_user`, `workflow` |
| 2 | Depend on Tier 0-1 | 11 | `recipe`, `smurf_group`, `workflow_step` |
| 3 | Depend on Tier 0-2 (P0 CRITICAL) | 12 | `goo`, `fatsmurf`, `goo_attachment` |
| 4 | Depend on Tier 0-3 (P0 LINEAGE) | 11 | `material_transition`, `transition_material` |

---

## ‚úÖ Validation Criteria

### 1. Referential Integrity (PASS/FAIL)

- **Status:** `‚úì PASS` if 0 orphaned FK rows
- **Script:** `validate-referential-integrity.sql`
- **Critical Checks:**
  - All 121 FK constraints
  - UID-based FKs: `material_transition`, `transition_material`

### 2. Row Count Variance (¬±2% acceptable)

- **Target:** 15% ¬±2% (13-17% acceptable)
- **Script:** `validate-row-counts.sql`
- **Status:**
  - `‚úì PASS`: 13-17%
  - `‚ö† WARNING`: 10-13% or 17-20%
  - `‚úó FAIL`: <10% or >20%

### 3. Data Integrity Checksums (Manual Comparison)

- **Method:** MD5 checksums of 100-row samples
- **Script:** `validate-checksums.sql`
- **Tables:** 9 critical tables (P0 + key dependencies)
- **Validation:** Run equivalent queries on SQL Server, compare `aggregate_checksum`

---

## üêõ Troubleshooting

### Issue 1: "INSUFFICIENT TEMPDB SPACE" Error

**Symptom:** Script aborts with tempdb space error

**Cause:** SQL Server tempdb has <2GB free space

**Fix:**
```sql
-- Check tempdb free space
sqlcmd -S sqlapps -U sqlapps-repl -P 'password' -d tempdb -Q "
SELECT SUM(unallocated_extent_page_count) * 8 / 1024 AS free_mb
FROM tempdb.sys.dm_db_file_space_usage"

-- If < 2000 MB, expand tempdb or clean up temp tables
```

### Issue 2: Connection Timeout

**Symptom:** `sqlcmd: Error: Connection timeout`

**Causes:**
- Network connectivity issues
- SQL Server offline or restarting
- Firewall blocking port 1433

**Fix:**
```bash
# Test connectivity
ping sqlapps

# Test SQL Server port
telnet sqlapps 1433

# Verify credentials
sqlcmd -S sqlapps -U sqlapps-repl -P 'password' -Q "SELECT @@VERSION"
```

### Issue 3: BCP Export Failures

**Symptom:** CSV files missing or 0 bytes

**Causes:**
- Temp tables dropped before BCP export
- BCP permission issues
- Disk full

**Fix:**
```bash
# Check disk space
df -h /tmp

# Verify temp tables exist
sqlcmd -S sqlapps -U sqlapps-repl -P 'password' -d tempdb -Q "
SELECT name FROM tempdb.sys.tables WHERE name LIKE '##perseus_tier_%'"

# Re-run specific tier with --no-cleanup
./extract-data.sh --tier 2 --no-cleanup
```

### Issue 4: Emergency Stop (Kill Session)

**Symptom:** Extraction running too long, need to abort

**Action:**
```sql
-- Use Session ID from extract-data.sh output (e.g., 1234)
KILL 1234

-- Verify session killed
SELECT * FROM sys.dm_exec_sessions WHERE session_id = 1234
```

### Issue 5: Orphaned FK Rows

**Symptom:** `validate-referential-integrity.sql` reports failures

**Causes:**
- FK filter logic error in extraction scripts
- Incomplete tier execution (tier0 ran, but tier1 skipped)

**Fix:**
1. Drop all PostgreSQL data: `TRUNCATE TABLE perseus.* CASCADE;`
2. Re-run full extraction: `./extract-data.sh`
3. Validate again: `psql -f validate-referential-integrity.sql`

---

## üìä Expected Metrics (DEV Environment)

| Metric | Expected Value | Source |
|--------|----------------|--------|
| **Tables Loaded** | 76 | All tiers complete |
| **Total Rows** | ~125,000 | 15% of SQL Server source |
| **FK Constraints** | 121 | All should pass validation |
| **Orphaned Rows** | 0 | Referential integrity requirement |
| **Extraction Duration** | 5-15 minutes | Depends on network + server load |
| **CSV Export Duration** | 2-5 minutes | BCP performance |
| **Total CSV Size** | 800MB-1.5GB | 15% of production data |

**P0 Critical Tables (Row Count Examples):**

| Table | Est. Source Rows | Est. DEV Rows (15%) | Priority |
|-------|------------------|---------------------|----------|
| `goo` | ~500,000 | ~75,000 | P0 |
| `fatsmurf` | ~200,000 | ~30,000 | P0 |
| `material_transition` | ~1,000,000 | ~150,000 | P0 |
| `transition_material` | ~1,000,000 | ~150,000 | P0 |
| `goo_type` | ~500 | ~75 | P1 |
| `perseus_user` | ~200 | ~30 | P1 |

---

## üìù Success Checklist

Before marking data migration as complete:

- [ ] **Setup:** `.env` file created and secured (`chmod 600`)
- [ ] **Dry Run:** `./extract-data.sh --dry-run` passes all checks
- [ ] **Extraction:** `./extract-data.sh` completes successfully (all 5 tiers)
- [ ] **CSV Export:** 76 CSV files created in `/tmp/perseus-data-export/`
- [ ] **CSV Validation:** All CSVs have size >0 bytes
- [ ] **PostgreSQL Load:** `./load-data.sh` completes with 0 errors
- [ ] **Referential Integrity:** `validate-referential-integrity.sql` - ALL PASS (0 failures)
- [ ] **Row Counts:** `validate-row-counts.sql` - 15% ¬±2% variance
- [ ] **Checksums:** `validate-checksums.sql` - Manual comparison with SQL Server
- [ ] **P0 Critical:** `goo`, `fatsmurf`, `material_transition`, `transition_material` validated
- [ ] **Documentation:** Results logged in `tracking/activity-log-2026-01.md`
- [ ] **Stakeholder:** DBA approval for DEV data quality

---

## üîó Related Documentation

- **Project Overview:** `docs/PROJECT-SPECIFICATION.md`
- **User Story 3:** `specs/001-tsql-to-pgsql/tasks.md` (T126-T131)
- **Table Structures:** `docs/code-analysis/table-dependency-graph.md`
- **FK Constraint Fixes:** `docs/FK-CONSTRAINT-FIXES.md`
- **DEV Deployment:** `docs/DEV-DEPLOYMENT-COMPLETE.md`
- **Data Migration Plan:** `docs/DATA-MIGRATION-PLAN-DEV.md`
- **Production Safety Guide:** `docs/DATA-EXTRACTION-SCRIPTS-REVIEW.md`

---

## üìû Support

**Issues?** Contact:
- **DBA:** Pierre Ribeiro (Senior DBA/DBRE)
- **Project:** Perseus Database Migration (SQL Server ‚Üí PostgreSQL 17)
- **Tracker:** `tracking/progress-tracker.md`

**Version History:**
- **v3.0** (2026-01-29): Production-safe automation with extract-data.sh
- **v2.0** (2026-01-26): Corrected FK filtering and error handling
- **v1.0** (2026-01-25): Initial manual workflow

**Last Updated:** 2026-01-29 | **Status:** ‚úÖ Production-Ready
